"""
Description Augmentation Module using Large Language Models

This module provides functionality to enhance object descriptions by processing
multiple Multi-modal Large Language Model (MLLM) generated descriptions through
a Large Language Model (LLM). The module extracts commonalities from multiple
descriptions of the same object instance and generates improved, consolidated
descriptions suitable for visual grounding tasks.

The augmentation process involves:
1. Grouping MLLM descriptions by object instance
2. Extracting common characteristics using LLM reasoning
3. Generating enhanced descriptions in visual grounding format

"""

import sys
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import logging
import time

import pandas as pd
from openai import OpenAI


@dataclass
class AugmentationConfig:
    """
    Configuration class for the description augmentation process.
    
    Attributes:
        api_key (str): API key for the LLM service
        base_url (str): Base URL for the LLM API
        model_name (str): Name of the LLM model to use
        max_workers (int): Maximum number of concurrent threads
        timeout (int): Request timeout in seconds
        max_retries (int): Maximum number of retry attempts
    """
    api_key: str
    base_url: str
    model_name: str = "deepseek-chat"
    max_workers: int = 24
    timeout: int = 30
    max_retries: int = 3


@dataclass
class DescriptionGroup:
    """
    Data class representing a group of descriptions for a single object instance.
    
    Attributes:
        file_path (str): Path to the object instance image
        category (str): Object category/class name
        descriptions (List[str]): List of MLLM-generated descriptions
        filename (str): Extracted filename from file path
    """
    file_path: str
    category: str
    descriptions: List[str]
    filename: str


@dataclass
class AugmentationResult:
    """
    Data class storing the result of description augmentation.
    
    Attributes:
        file_path (str): Original file path
        original_descriptions (List[str]): Original MLLM descriptions
        augmented_description (str): Enhanced description generated by LLM
        processing_time (float): Time taken for processing
        success (bool): Whether augmentation was successful
        error_message (Optional[str]): Error message if processing failed
        category (str): Object category
    """
    file_path: str
    original_descriptions: List[str]
    augmented_description: str
    processing_time: float
    success: bool = True
    error_message: Optional[str] = None
    category: str = ""


class PromptGenerator:
    """
    Utility class for generating prompts for description augmentation.
    """
    
    @staticmethod
    def create_augmentation_prompt(category: str, descriptions: List[str]) -> str:
        """
        Create a prompt for description augmentation based on category and descriptions.
        
        Args:
            category (str): Object category name
            descriptions (List[str]): List of original descriptions
            
        Returns:
            str: Generated prompt for LLM
        """
        template = (
            "Extract the commonalities from the following descriptions of {} and generate "
            "a new consolidated description for this object, similar to the descriptions "
            "in the Visual Grounding dataset. Focus on the most distinctive and consistent "
            "visual characteristics. No explanation is needed, just generate the description.\n\n{}"
        )
        
        # Clean and format descriptions
        cleaned_descriptions = [desc.strip() for desc in descriptions if desc.strip()]
        descriptions_text = '\n'.join(f"- {desc}" for desc in cleaned_descriptions)
        
        return template.format(category, descriptions_text)
    
    @staticmethod
    def create_system_prompt() -> str:
        """
        Create system prompt for the LLM.
        
        Returns:
            str: System prompt text
        """
        return (
            "You are an expert in computer vision and natural language processing. "
            "Your task is to analyze multiple object descriptions and create a single, "
            "comprehensive description that captures the most important and consistent "
            "visual characteristics. Focus on attributes that would be useful for "
            "visual grounding tasks."
        )


class DescriptionAugmentor:
    """
    Main class for augmenting object descriptions using Large Language Models.
    
    This class processes multiple MLLM-generated descriptions for each object instance,
    extracts commonalities using LLM reasoning, and generates enhanced descriptions
    suitable for visual grounding applications.
    """
    
    def __init__(self, config: AugmentationConfig):
        """
        Initialize the Description Augmentor.
        
        Args:
            config (AugmentationConfig): Configuration for the augmentation process
        """
        self.config = config
        self.client = OpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
            timeout=config.timeout
        )
        
        # Thread-safe progress tracking
        self.progress_lock = threading.Lock()
        self.completed_tasks = 0
        self.total_tasks = 0
        self.results = []
        
        # Setup logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # Initialize prompt generator
        self.prompt_generator = PromptGenerator()
    
    def _extract_category_from_filename(self, filename: str) -> str:
        """
        Extract object category from filename.
        
        Args:
            filename (str): Name of the image file
            
        Returns:
            str: Extracted category name
        """
        return filename.split("_")[0] if "_" in filename else filename.split(".")[0]
    
    def _parse_file_path(self, file_path: str) -> str:
        """
        Parse file path to extract filename.
        
        Args:
            file_path (str): Full file path
            
        Returns:
            str: Extracted filename
        """
        # Handle both Windows and Unix path separators
        separators = ['\\', '/']
        filename = file_path
        
        for sep in separators:
            if sep in filename:
                filename = filename.split(sep)[-1]
        
        return filename
    
    def load_and_group_descriptions(self, csv_path: str) -> List[DescriptionGroup]:
        """
        Load descriptions from CSV and group by file path.
        
        Args:
            csv_path (str): Path to CSV file containing MLLM descriptions
            
        Returns:
            List[DescriptionGroup]: Grouped descriptions by object instance
            
        Raises:
            FileNotFoundError: If CSV file doesn't exist
            ValueError: If CSV has invalid format
        """
        try:
            # Load CSV file
            df = pd.read_csv(csv_path)
            
            # Validate required columns
            required_columns = ['file_path', 'description']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise ValueError(f"CSV missing required columns: {missing_columns}")
            
            # Remove empty descriptions
            df = df.dropna(subset=['description'])
            df = df[df['description'].str.strip() != '']
            
            # Group descriptions by file path
            grouped = df.groupby('file_path')['description'].apply(list).reset_index()
            
            description_groups = []
            for _, row in grouped.iterrows():
                file_path = row['file_path']
                descriptions = row['description']
                filename = self._parse_file_path(file_path)
                category = self._extract_category_from_filename(filename)
                
                description_groups.append(DescriptionGroup(
                    file_path=file_path,
                    category=category,
                    descriptions=descriptions,
                    filename=filename
                ))
            
            self.total_tasks = len(description_groups)
            self.logger.info(f"Loaded {len(description_groups)} description groups from {csv_path}")
            
            return description_groups
            
        except FileNotFoundError:
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        except Exception as e:
            raise ValueError(f"Error loading CSV file: {e}")
    
    def _make_llm_request(self, prompt: str, system_prompt: str) -> str:
        """
        Make a request to the LLM with retry logic.
        
        Args:
            prompt (str): User prompt
            system_prompt (str): System prompt
            
        Returns:
            str: Generated response
            
        Raises:
            Exception: If all retry attempts fail
        """
        for attempt in range(self.config.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.config.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt},
                    ],
                    stream=False
                )
                
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise e
                
                wait_time = (attempt + 1) * 2  # Exponential backoff
                self.logger.warning(f"Request failed (attempt {attempt + 1}), retrying in {wait_time}s: {e}")
                time.sleep(wait_time)
        
        raise Exception("All retry attempts failed")
    
    def process_description_group(self, desc_group: DescriptionGroup) -> AugmentationResult:
        """
        Process a single description group to generate augmented description.
        
        Args:
            desc_group (DescriptionGroup): Group of descriptions for one object
            
        Returns:
            AugmentationResult: Result of the augmentation process
        """
        start_time = time.time()
        
        try:
            # Generate prompts
            user_prompt = self.prompt_generator.create_augmentation_prompt(
                desc_group.category, desc_group.descriptions
            )
            system_prompt = self.prompt_generator.create_system_prompt()
            
            # Make LLM request
            augmented_description = self._make_llm_request(user_prompt, system_prompt)
            
            processing_time = time.time() - start_time
            
            # Update progress
            with self.progress_lock:
                self.completed_tasks += 1
                progress_percent = (self.completed_tasks / self.total_tasks) * 100
                self.logger.info(
                    f"[{self.completed_tasks}/{self.total_tasks}] ({progress_percent:.1f}%) "
                    f"Processed: {desc_group.file_path}"
                )
            
            return AugmentationResult(
                file_path=desc_group.file_path,
                original_descriptions=desc_group.descriptions,
                augmented_description=augmented_description,
                processing_time=processing_time,
                success=True,
                category=desc_group.category
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            
            with self.progress_lock:
                self.completed_tasks += 1
            
            self.logger.error(f"Error processing {desc_group.file_path}: {e}")
            
            return AugmentationResult(
                file_path=desc_group.file_path,
                original_descriptions=desc_group.descriptions,
                augmented_description="",
                processing_time=processing_time,
                success=False,
                error_message=str(e),
                category=desc_group.category
            )
    
    def augment_all_descriptions(self, description_groups: List[DescriptionGroup]) -> List[AugmentationResult]:
        """
        Process all description groups using concurrent processing.
        
        Args:
            description_groups (List[DescriptionGroup]): List of description groups to process
            
        Returns:
            List[AugmentationResult]: Results from all processing tasks
        """
        self.logger.info(f"Starting augmentation of {len(description_groups)} groups with {self.config.max_workers} workers")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # Submit all tasks
            future_to_group = {
                executor.submit(self.process_description_group, group): group 
                for group in description_groups
            }
            
            # Collect results
            for future in as_completed(future_to_group):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    group = future_to_group[future]
                    self.logger.error(f"Unexpected error processing {group.file_path}: {e}")
                    
                    # Create failed result
                    results.append(AugmentationResult(
                        file_path=group.file_path,
                        original_descriptions=group.descriptions,
                        augmented_description="",
                        processing_time=0,
                        success=False,
                        error_message=str(e),
                        category=group.category
                    ))
        
        return results
    
    def save_results(self, results: List[AugmentationResult], output_path: str) -> None:
        """
        Save augmentation results to CSV file.
        
        Args:
            results (List[AugmentationResult]): Results to save
            output_path (str): Output CSV file path
        """
        try:
            # Prepare data for DataFrame
            output_data = []
            for result in results:
                output_data.append({
                    'file_path': result.file_path,
                    'category': result.category,
                    'original_description_count': len(result.original_descriptions),
                    'augmented_description': result.augmented_description,
                    'processing_time': result.processing_time,
                    'success': result.success,
                    'error_message': result.error_message or "",
                    'original_descriptions': " | ".join(result.original_descriptions)
                })
            
            # Create DataFrame and save
            output_df = pd.DataFrame(output_data)
            output_df.to_csv(output_path, index=False, encoding='utf-8')
            
            self.logger.info(f"Results saved to: {output_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to save results: {e}")
            raise
    
    def generate_statistics(self, results: List[AugmentationResult]) -> Dict[str, Any]:
        """
        Generate processing statistics from results.
        
        Args:
            results (List[AugmentationResult]): Processing results
            
        Returns:
            Dict[str, Any]: Statistics summary
        """
        successful_results = [r for r in results if r.success]
        failed_results = [r for r in results if not r.success]
        
        # Calculate statistics
        total_processing_time = sum(r.processing_time for r in results)
        avg_processing_time = total_processing_time / len(results) if results else 0
        
        # Category-wise statistics
        category_stats = {}
        for result in results:
            if result.category not in category_stats:
                category_stats[result.category] = {'total': 0, 'successful': 0}
            category_stats[result.category]['total'] += 1
            if result.success:
                category_stats[result.category]['successful'] += 1
        
        statistics = {
            'total_processed': len(results),
            'successful': len(successful_results),
            'failed': len(failed_results),
            'success_rate': (len(successful_results) / len(results) * 100) if results else 0,
            'total_processing_time': total_processing_time,
            'average_processing_time': avg_processing_time,
            'category_statistics': category_stats
        }
        
        return statistics
    
    def generate_summary_report(self, statistics: Dict[str, Any], results: List[AugmentationResult]) -> str:
        """
        Generate a comprehensive summary report.
        
        Args:
            statistics (Dict[str, Any]): Processing statistics
            results (List[AugmentationResult]): Processing results
            
        Returns:
            str: Formatted summary report
        """
        report = f"""
        Description Augmentation Summary Report
        ======================================

        Configuration:
        - Model: {self.config.model_name}
        - Max Workers: {self.config.max_workers}
        - Max Retries: {self.config.max_retries}

        Processing Results:
        - Total Instances: {statistics['total_processed']}
        - Successfully Processed: {statistics['successful']}
        - Failed Processing: {statistics['failed']}
        - Success Rate: {statistics['success_rate']:.1f}%

        Performance Metrics:
        - Total Processing Time: {statistics['total_processing_time']:.1f} seconds
        - Average Processing Time: {statistics['average_processing_time']:.3f} seconds
        - Throughput: {statistics['total_processed'] / max(statistics['total_processing_time'], 1):.2f} instances/second

        Category-wise Statistics:
        """
        
        for category, stats in statistics['category_statistics'].items():
            success_rate = (stats['successful'] / stats['total'] * 100) if stats['total'] > 0 else 0
            report += f"- {category}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)\n"
        
        # Add error analysis if there are failures
        failed_results = [r for r in results if not r.success]
        if failed_results:
            report += f"\nError Analysis:\n"
            error_counts = {}
            for result in failed_results:
                error_type = type(Exception(result.error_message)).__name__ if result.error_message else "Unknown"
                error_counts[error_type] = error_counts.get(error_type, 0) + 1
            
            for error_type, count in error_counts.items():
                report += f"- {error_type}: {count} occurrences\n"
        
        return report


def main():
    """
    Main function to execute the description augmentation pipeline.
    """
    # Configuration parameters
    INPUT_CSV = "mllm_grounding_results.csv"  # Update with your CSV path
    OUTPUT_CSV = "description_augmentation_results.csv"
    
    # LLM Configuration
    config = AugmentationConfig(
        api_key="",  # Add your API key here
        base_url="https://api.deepseek.com",
        model_name="deepseek-chat",
        max_workers=24,
        timeout=30,
        max_retries=3
    )
    
    try:
        print("üîÑ Starting description augmentation pipeline...")
        
        # Initialize augmentor
        augmentor = DescriptionAugmentor(config)
        
        # Load and group descriptions
        description_groups = augmentor.load_and_group_descriptions(INPUT_CSV)
        
        # Process all groups
        results = augmentor.augment_all_descriptions(description_groups)
        
        # Save results
        augmentor.save_results(results, OUTPUT_CSV)
        
        # Generate statistics and report
        statistics = augmentor.generate_statistics(results)
        report = augmentor.generate_summary_report(statistics, results)
        
        print(report)
        
        print("‚úÖ Description augmentation completed successfully!")
        print(f"üìä Successfully processed: {statistics['successful']}/{statistics['total_processed']} instances")
        print(f"üìÅ Results saved to: {OUTPUT_CSV}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
